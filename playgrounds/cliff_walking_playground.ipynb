{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cliff Walking Playground\n",
    "Playground used to experiment with different algorithms on the Cliff Walking environment from Example 6.6 of Sutton and Barto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/Users/bogdanfloris/Downloads/Code/Reinforcement-Learning/')\n",
    "from library.dynamic_programming import dynamic_programming as dp\n",
    "from library.td_learning import temporal_diff_learning as td\n",
    "from library.environments.cliff_walking import CliffWalkingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming\n",
    "Experiments on Cliff Walking environment using Dynamic Programming algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "policy, state_values = dp.policy_iteration(env=env, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy found using Policy Iteration algorithm\n",
      " →  →  →  →  →  →  →  →  →  →  →  ↓ \n",
      " →  →  →  →  →  →  →  →  →  →  →  ↓ \n",
      " →  →  →  →  →  →  →  →  →  →  →  ↓ \n",
      " ↑  C  C  C  C  C  C  C  C  C  C  G "
     ]
    }
   ],
   "source": [
    "print(\"Optimal policy found using Policy Iteration algorithm\")\n",
    "env.render_policy(policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration converges to the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Difference Learning\n",
    "Experiments on Cliff Walking environment using Temporal Difference Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA hyperparameters\n",
    "num_episodes = 1000\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy after 1000 episodes of SARSA for ε = 0.1\n",
      " →  →  →  →  →  →  →  →  →  →  →  ↓ \n",
      " →  ↑  →  ↑  ←  ↑  ↑  ↑  ↑  ↑  →  ↓ \n",
      " ↑  ↑  ↑  ↑  ↑  ←  ↑  →  ↑  ←  →  ↓ \n",
      " ↑  C  C  C  C  C  C  C  C  C  C  G "
     ]
    }
   ],
   "source": [
    "q, _ = td.sarsa(env, num_episodes, epsilon=epsilon)\n",
    "policy = td.make_epsilon_greedy_policy(q=q, epsilon=0.0, action_count=env.action_space.n)\n",
    "print(\"Policy after {} episodes of {} for \\u03B5 = {}\".format(num_episodes, 'SARSA', epsilon))\n",
    "env.render_policy(policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning hyperparameters\n",
    "num_episodes = 1000\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy after 1000 episodes of Q-Learning for ε = 0.1\n",
      " ↑  →  →  ↓  →  →  ↓  ←  ↓  ↑  ↓  ↓ \n",
      " ↓  →  →  →  →  ↓  ↓  →  →  ↓  ↓  ↓ \n",
      " →  →  →  →  →  →  →  →  →  →  →  ↓ \n",
      " ↑  C  C  C  C  C  C  C  C  C  C  G "
     ]
    }
   ],
   "source": [
    "q, _ = td.q_learning(env, num_episodes, epsilon=epsilon)\n",
    "policy = td.make_epsilon_greedy_policy(q=q, epsilon=0.0, action_count=env.action_space.n)\n",
    "print(\"Policy after {} episodes of {} for \\u03B5 = {}\".format(num_episodes, 'Q-Learning', epsilon))\n",
    "env.render_policy(policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, SARSA chooses to take the top route, while Q-Learning chooses the route that is near the cliff (taking the risk of falling in it, if we make an epsilon greedy policy). These results are according to the Sutton and Barto example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
